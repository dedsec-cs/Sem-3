## 1. Introduction to Machine Learning

Machine Learning (ML) is a branch of **Artificial Intelligence (AI)** that enables machines to _learn patterns from data_ and improve performance based on experience â€” **without being explicitly programmed**.

> Traditional programming = Rules + Data â†’ Output  
> Machine Learning = Data + Output â†’ Algorithm learns rules

ML algorithms learn a function:

[  
f(X) --> Y  
]

Where:

- ( X ) = input features
    
- ( Y ) = target/output we want to predict
    

---

### Why ML is needed?

- Impossible to manually define rules for large/complex data (Face recognition, Spam filtering)
    
- Massive availability of structured & unstructured data
    
- ML models adapt as patterns change
    

---

### ML Applications

- Fraud detection (finance)
    
- Disease prediction (healthcare)
    
- Self-driving cars (reinforcement learning)
    
- Recommendation systems (Netflix, Amazon)
    

---

### ğŸ“Œ Diagram: Machine Learning Overview

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Machine Learning     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                       â”‚                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Supervised    â”‚      â”‚ Unsupervised  â”‚       â”‚ Reinforcement  â”‚
â”‚ (Labeled Data)â”‚      â”‚ (Unlabeled)   â”‚       â”‚ Learning        â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                        â”‚                        â”‚
      â”‚                        â”‚                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Regression  â”‚         â”‚ Clustering  â”‚         â”‚ Agent interacts â”‚
â”‚ (Predict #) â”‚         â”‚ (Groups)    â”‚         â”‚ + learns policy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Classification â”‚
â”‚ (Predict Class)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Types of Machine Learning

### âœ… 2.1 Supervised Learning

- Uses **labeled data**
    
- Learns mapping between input features ( X ) and target ( Y )
    

Examples:

- Predicting house price â†’ Regression
    
- Classifying emails (Spam/Not spam) â†’ Classification
    

---

### âœ… 2.2 Unsupervised Learning

- Uses **unlabeled data**
    
- Model discovers hidden patterns or structures
    

Examples:

- Customer segmentation (Clustering)
    
- Dimensionality reduction â†’ PCA
    

---

### âœ… 2.3 Reinforcement Learning

- System (agent) interacts with environment
    
- Learns a strategy (policy) based on **reward / penalty**
    

Examples:

- Robotics movement
    
- Game playing (Chess / Pac-Man)
    

---

### ğŸ“Œ Diagram: Comparison

```
Supervised        â†’ learns from labeled data (X, Y)
Unsupervised      â†’ learns from structure of X only
Reinforcement     â†’ learns by interacting (actions â†’ reward)
```

---

## 3. Feature Engineering 

Feature Engineering is **the art and science of converting raw data into meaningful features** that improve model performance.

> â€œBetter features â†’ better results, even with simple models.â€

---

### Why Feature Engineering is Important?

|Benefit|Reason|
|---|---|
|Improves accuracy|Features carry key information|
|Reduces overfitting|Removes irrelevant/noisy data|
|Faster training|Fewer features = less computation|
|Improves interpretability|Helps explain model decisions|

---

### Major Steps in Feature Engineering

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   RAW DATA (collected)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ 1. Data Cleaning                   â”‚
          â”‚ remove duplicates, fix formatting  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ 2. Handling Missing Data            â”‚
          â”‚ drop / impute / flag                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ 3. Encoding Categorical Data        â”‚
          â”‚ Label / One-Hot / Target encoding   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ 4. Feature Scaling                  â”‚
          â”‚ Z-score / Min-Max / Robust scaling  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ 5. Selection / Extraction           â”‚
          â”‚ Reduce features (Filter / PCA)      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Features and Their Types

|Type|Meaning|Examples|
|---|---|---|
|**Numerical**|Measurable numeric values|Age, Salary, Height|
|**Categorical**|Values represent categories|Gender, City|
|**Ordinal**|Categories with order|Low < Medium < High|
|**Text Features**|Derived from text|Word count, TF-IDF|
|**Datetime Features**|Derived from timestamps|Year, Month, Weekend|

---

ğŸ“Œ **Diagram: Types of Features**

```
                       Features
                           â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                     â”‚                     â”‚
 Numerical           Categorical              Datetime
     â”‚                     â”‚                     â”‚
 â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”              â””â”€> year, month,
 â”‚Discrete â”‚         â”‚Nominal   â”‚                 weekday, etc.
 â”‚Continuousâ”‚        â”‚Ordinal   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Handling Missing Data (Very Important)

Missing values negatively affect algorithms.

### Causes of Missing Data

- System failures (sensor not capturing value)
    
- Human entry mistake
    
- Value not applicable
    

---

### Strategies to Handle Missing Data

|Method|When to Use|
|---|---|
|**Remove rows**|When only few rows are missing|
|**Remove column**|When > 60% values are missing|
|**Impute (fill values)**|For essential features|
|**Flag missingness**|When missing value itself is meaningful|

---

ğŸ“Œ **Diagram: Missing Data Decision**

```
                       Missing Data?
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        â”‚                   â”‚          â”‚
        Drop rows   Drop column     Impute values   Flag missing
        (rare)      (too many NA)   (mean/median)   (add feature)
```

---

## 6. Dealing with Categorical Features (Encoding)

ML algorithms **require numeric data**, so categorical values must be converted.

|Encoding|Use Case|
|---|---|
|**Label Encoding**|Ordinal categories (rating: low < medium < high)|
|**One-Hot Encoding (OHE)**|Nominal categories (city, gender)|
|**Target Encoding**|High-cardinality categories (zip code)|
|**Frequency Encoding**|Categories represent count behavior|

---

ğŸ“Œ **Diagram: Encoding Selection**

```
                   Categorical Feature
                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚           â”‚               â”‚             â”‚
   Is Ordered?    Few Categories?   Many?     Target correlated?
      â”‚               â”‚              â”‚               â”‚
Label Encoding     One-Hot        Frequency      Target Encoding
```

# ğŸ”µ DEALING WITH CATEGORICAL FEATURES (Encoding)

> ğŸ“Œ From PPT:
> 
> - _â€œCategorical variables need to be converted to numerical values before modeling.â€_
>     
> - _â€œEncoding is required because ML algorithms cannot understand text values.â€_
>     
> - _â€œTwo types of categorical data: Nominal and Ordinal.â€_
>     

Categorical features represent labels or groups.  
They canâ€™t be directly fed into ML models because models perform numeric computations.

Examples from PPT:

- Gender (Male/Female)
    
- Stream (Arts/Commerce/Science)
    

---

### â¤ Types of Categorical Features (PPT Mapping)

|PPT Mentioned|Meaning|Examples|
|---|---|---|
|**Nominal**|No ordering between categories|City, Gender, Country|
|**Ordinal**|Ordering exists|Low < Medium < High (rating), Class levels|

---

### â¤ PPT: â€œMethods to encode categorical dataâ€

```
               Handling Categorical Data
                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚                  â”‚
 Label Encoding     One Hot Encoding   Target/Frequency Encoding
```

---

### âœ… Label Encoding

> PPT point: _â€œAssigns numerical values to categories.â€_

Use for: **Ordinal Data**

Example from PPT (slide with ranking example):

```
Low â†’ 0
Medium â†’ 1
High â†’ 2
```

---

### âœ… One-Hot Encoding

> PPT point: _â€œCreates binary columns for each category.â€_

Use for: **Nominal Data**

```
Gender:
Male   â†’ [1 0]
Female â†’ [0 1]
```

ğŸ”¹ PPT mentioned the problem of **dimensionality increase** when many categories exist.

---

### âœ… Target / Frequency Encoding

> PPT point: _â€œUsed when categories are too many.â€_

Use when a feature contains **hundreds or thousands of categories**, e.g., ZIP codes, product IDs.

---

---

# ğŸ”µ FEATURE SCALING

> ğŸ“Œ From PPT:
> 
> - _â€œScaling ensures all features contribute equally to the result.â€_
>     
> - _â€œRequired when data varies in magnitude.â€_
>     
> - _â€œImproves accuracy of distance-based algorithms.â€_
>     

Scaling makes numerical features fall within **similar ranges**, avoiding bias toward features with large values.

Example from PPT:

- Age â†’ 18 to 60
    
- Salary â†’ 30,000 to 120,000
    

---

### â¤ PPT: â€œWhen scaling is needed?â€

Algorithms requiring scaling (PPT list matches this):

```
âœ” K-Means
âœ” K-Nearest Neighbors (KNN)
âœ” Support Vector Machine (SVM)
âœ” Logistic Regression
âœ” Neural Networks
âœ” PCA (MUST scale before PCA)
```

Algorithms where scaling is **not required** (from PPT):

```
âœ˜ Decision Trees
âœ˜ Random Forest
âœ˜ XGBoost
```

---

### â¤ PPT: "Types of Scaling"

|Type (as in PPT)|Explanation|Where used|
|---|---|---|
|Standardization|Centers data|Most ML algorithms|
|Normalization|Converts to fixed range (0â€“1)|Neural networks|
|Robust Scaling|Uses median; handles outliers|Data with outliers|

---

### ğŸ“Œ PPT Included Example

> "After scaling, all values lie in a similar range, improving model performance."

---

---

# ğŸ”µ FEATURE SELECTION

> ğŸ“Œ From PPT:
> 
> - _â€œUsed to reduce overfitting and increase accuracy.â€_
>     
> - _â€œRemoves irrelevant or redundant features.â€_
>     
> - _â€œReduces computation time.â€_
>     

Feature Selection identifies **the most useful features**.

---

### â¤ PPT: â€œWhy do we select features?â€

|PPT Benefit|Meaning|
|---|---|
|Reduce overfitting|Remove noise + unnecessary columns|
|Faster training|Less data â†’ faster computation|
|Improves model accuracy|More relevant inputs = better model|

---

### â¤ PPT Categorization

```
                Feature Selection
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
      Filter         Wrapper        Embedded
```

---

### âœ… Filter Methods (PPT)

- Based on **statistics**
    
- Fastest method
    

PPT examples:

- Correlation matrix
    
- Chi-square test
    

---

### âœ… Wrapper Methods (PPT)

- Try subsets of features by _training the model repeatedly_
    

Examples:

- RFE (Recursive Feature Elimination)
    
- Forward selection
    
- Backward elimination
    

---

### âœ… Embedded Methods (PPT)

- Feature selection happens **during training**
    

Examples from PPT:

- LASSO (L1 feature elimination)
    
- Decision Tree Feature Importance
    

---

---

# ğŸ”µ FEATURE EXTRACTION (PCA Algorithm)

> ğŸ“Œ From PPT:
> 
> - _"PCA reduces dimensionality while retaining maximum information."_
>     
> - _"Transforms correlated variables into new uncorrelated components."_
>     
> - _"Helps improve model efficiency."_
>     

**Feature Selection = remove columns**  
**Feature Extraction = convert existing columns into new meaningful columns**

---

## âœ… PCA (Principal Component Analysis)

PPT summary:

- Used to reduce number of features
    
- Makes visualization possible (2D/3D)
    
- Converts correlated features into new independent components (PC1, PC2 â€¦)
    

---

### â¤ PPT: â€œSteps in PCAâ€

```
Step 1: Standardize data
Step 2: Calculate the covariance matrix
Step 3: Identify principal components
Step 4: Sort components (highest information first)
Step 5: Transform original data into new components (PC1, PC2, ...)
```

---

### ğŸ“Œ PPT diagram: High-dimensional â†’ Compact representation

```
Original Data (multiple features)
       â†“
Find directions of maximum variation
       â†“
Project data onto new axes (Principal Components)
       â†“
Reduced and meaningful data representation
```

---

### Important PPT notes:

- PCA **must** be applied **after scaling**
    
- PCA is used when dataset has many correlated features
    
- PCA exchanged interpretability for performance
    

---

## âœ… Difference (as per PPT wording)

|Feature Selection|PCA (Feature Extraction)|
|---|---|
|Removes unnecessary features|Creates new transformed features|
|Keeps original meaning|Loses original meaning|
|LASSO, RFE|PCA|

---

**Short (2â€“4 marks):**

- Define feature engineering.
    
- Explain One-Hot Encoding with an example.
    
- What is PCA and why is it used?
    
- Differentiate numerical and categorical features.
    

**Long (10â€“12 marks):**

- Explain principal component analysis with steps and diagrams.
    
- How do we handle missing data? Explain types and techniques.
    
- Discuss Feature Engineering pipeline in detail.
    
- Explain feature scaling and feature selection with examples.
    

---



